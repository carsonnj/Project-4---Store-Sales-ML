{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carsonnj/Project-4---Store-Sales-ML/blob/main/Store_Sales_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rHd_WCDTYZ2R"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from pathlib import Path\n",
        "!pip install hvplot\n",
        "import hvplot.pandas\n",
        "!pip install holoviews\n",
        "import holoviews as hv\n",
        "from holoviews import opts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfrAvRmQUns8",
        "outputId": "2e333252-2122-44c1-9e78-3ca74f050297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hvplot in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: bokeh>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from hvplot) (3.3.4)\n",
            "Requirement already satisfied: colorcet>=2 in /usr/local/lib/python3.10/dist-packages (from hvplot) (3.1.0)\n",
            "Requirement already satisfied: holoviews>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from hvplot) (1.17.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from hvplot) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from hvplot) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hvplot) (23.2)\n",
            "Requirement already satisfied: panel>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from hvplot) (1.3.8)\n",
            "Requirement already satisfied: param<3.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from hvplot) (2.0.2)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh>=1.0.0->hvplot) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=1.0.0->hvplot) (1.2.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh>=1.0.0->hvplot) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh>=1.0.0->hvplot) (6.0.1)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=1.0.0->hvplot) (6.3.3)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=1.0.0->hvplot) (2023.10.1)\n",
            "Requirement already satisfied: pyviz-comms>=0.7.4 in /usr/local/lib/python3.10/dist-packages (from holoviews>=1.11.0->hvplot) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->hvplot) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->hvplot) (2023.4)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from panel>=0.11.0->hvplot) (3.5.2)\n",
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=0.11.0->hvplot) (3.0.0)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=0.11.0->hvplot) (2.0.3)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from panel>=0.11.0->hvplot) (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from panel>=0.11.0->hvplot) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from panel>=0.11.0->hvplot) (4.66.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from panel>=0.11.0->hvplot) (6.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from panel>=0.11.0->hvplot) (4.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh>=1.0.0->hvplot) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->hvplot) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->panel>=0.11.0->hvplot) (0.5.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py->panel>=0.11.0->hvplot) (1.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py->panel>=0.11.0->hvplot) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->panel>=0.11.0->hvplot) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->panel>=0.11.0->hvplot) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->panel>=0.11.0->hvplot) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->panel>=0.11.0->hvplot) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.x  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.4.0'\n",
        "spark_version = 'spark-3.5.1'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "V719QN8lfqmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_sales_df = pd.read_csv(\"https://raw.githubusercontent.com/carsonnj/Project-4---Store-Sales-ML/main/Stores.csv\")\n",
        "store_sales_df.head()"
      ],
      "metadata": {
        "id": "8CuNI3bmIHj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Store_Area from square yards to square feet\n",
        "yards_to_ft = 9\n",
        "store_sales_df['sq_ft'] = store_sales_df['Store_Area'] * yards_to_ft\n",
        "\n",
        "# Display the first five rows of the dataframe\n",
        "store_sales_df.head()"
      ],
      "metadata": {
        "id": "IAsVf7SfTbra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Divide Sales/sq. feet for sales $ per sq. ft\n",
        "store_sales_df['sales_per_customer'] = store_sales_df['Store_Sales'] / store_sales_df['Daily_Customer_Count']\n",
        "#Show df\n",
        "store_sales_df.head()"
      ],
      "metadata": {
        "id": "RfA85Y79QiE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Divide Sales/sq. feet for sales $ per sq. ft\n",
        "store_sales_df['sales_per_sq_ft'] = store_sales_df['Store_Sales'] / store_sales_df['sq_ft']\n",
        "#Show df\n",
        "store_sales_df.head()"
      ],
      "metadata": {
        "id": "sGIATuBVUDiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering Model"
      ],
      "metadata": {
        "id": "c2h7IsZBUpRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Create a a list to store inertia values and the values of k\n",
        "inertia = []\n",
        "k = list(range(1, 11))\n",
        " # Create a for-loop where each value of k is evaluated using the K-means algorithm\n",
        "# Fit the model using the service_ratings DataFrame\n",
        "# Append the value of the computed inertia from the `inertia_` attribute of the KMeans model instance\n",
        "for i in k:\n",
        "    k_model = KMeans(n_clusters=i, random_state=1)\n",
        "    k_model.fit(store_sales_df)\n",
        "    inertia.append(k_model.inertia_)\n",
        "# Define a DataFrame to hold the values for k and the corresponding inertia\n",
        "elbow_data = {\"k\": k, \"inertia\": inertia}\n",
        "df_elbow = pd.DataFrame(elbow_data)\n",
        "\n",
        "# Review the DataFrame\n",
        "df_elbow.head()"
      ],
      "metadata": {
        "id": "oxSOv07_VqKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_6['inertia'].plot(kind='line', figsize=(8, 4), title='inertia')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "l0ULZofFvkjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the DataFrame\n",
        "df_elbow.hvplot.line(\n",
        "    x=\"k\",\n",
        "    y=\"inertia\",\n",
        "    title=\"Elbow Curve\",\n",
        "    xticks=k\n",
        ")\n"
      ],
      "metadata": {
        "id": "DLYwhuMUWIA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model with the lower value of k clusters\n",
        "# Use a random_state of 1 to generate the model\n",
        "model = KMeans(n_clusters=4, random_state=1)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(store_sales_df)\n",
        "\n",
        "# Make predictions\n",
        "k_lower = model.predict(store_sales_df)\n",
        "\n",
        "# Create a copy of the DataFrame\n",
        "store_sales_cluster_df = store_sales_df.copy()\n",
        "\n",
        "# Add a class column with the labels to the spread_df_predictions DataFrame\n",
        "store_sales_cluster_df['cluster_number'] = k_lower\n",
        "\n",
        "#Print prediction\n",
        "store_sales_cluster_df.head()"
      ],
      "metadata": {
        "id": "m5p410lYbW3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Start Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg\n",
        "# Create a SparkSession object\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "# Create a DataFrame from the existing store_sales_cluster_df variable\n",
        "store_sales_cluster_df = spark.createDataFrame(store_sales_cluster_df)\n",
        "#create temp table to use spark.sql to understand cluster\n",
        "store_sales_cluster_df.createOrReplaceTempView(\"cluster\")\n",
        "#group by cluster and look at average sales, sq. ft, and sales per sq. ft\n",
        "store_sales_cluster_df.groupBy(\"cluster_number\").agg(avg(\"Store_Sales\"), avg(\"Sq_Ft\"), avg(\"Sales_Per_Sq_Ft\"), avg(\"Items_Available\"), avg(\"Daily_Customer_Count\"), avg(\"sales_per_customer\")).show()"
      ],
      "metadata": {
        "id": "3tDXQXvhdm8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_sales_df.describe()"
      ],
      "metadata": {
        "id": "jrRYswR8ZsZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering to Optimize models"
      ],
      "metadata": {
        "id": "O3ceOjnCU-kX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# copy DF\n",
        "store_sales_copy_df = store_sales_df.copy()\n",
        "#Remove stores with less than 100 daily customerss\n",
        "store_sales_copy_df = store_sales_copy_df[store_sales_copy_df['Daily_Customer_Count'] >= 250]\n",
        "#Add Customers/Items\n",
        "store_sales_copy_df['customer_per_item'] = store_sales_copy_df['Daily_Customer_Count'] /store_sales_copy_df['Items_Available']\n",
        "#Add Items/sq. ft\n",
        "store_sales_copy_df['items_per_sq_ft'] = store_sales_copy_df['Items_Available']/store_sales_copy_df['sq_ft']\n",
        "#Add Customers/sq. ft\n",
        "store_sales_copy_df['customer_per_sq_ft'] = store_sales_copy_df['Daily_Customer_Count'] /store_sales_copy_df['sq_ft']\n",
        "store_sales_copy_df.describe()\n"
      ],
      "metadata": {
        "id": "kFWkeFLHUFO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Data distribution"
      ],
      "metadata": {
        "id": "QLiORTJAjh6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution of items avaliable\n",
        "store_sales_copy_df['Items_Available'].plot(kind='hist', bins=20, title='Items_Available')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)\n"
      ],
      "metadata": {
        "id": "9yLbyyHljgq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution of store sales\n",
        "store_sales_copy_df['Store_Sales'].plot(kind='hist', bins=20, title='Store_Sales')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)\n"
      ],
      "metadata": {
        "id": "5l9Ff65Kj87x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter Plot with items and store sales to see if there is a linear correlation\n",
        "plt.scatter(store_sales_copy_df['Items_Available'], store_sales_copy_df['Store_Sales'])\n",
        "\n",
        "# Set the title and axis labels\n",
        "plt.title('Sales vs. Items Available')\n",
        "plt.xlabel('Items Available')\n",
        "plt.ylabel('Sales')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "F048rG19kFny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter Plot with sq. ft and sales to see if there is a linear correlation\n",
        "plt.scatter(store_sales_df['sq_ft'], store_sales_df['Store_Sales'])\n",
        "\n",
        "# Set the title and axis labels\n",
        "plt.title('Sales vs. Square Feet')\n",
        "plt.xlabel('Square Feet')\n",
        "plt.ylabel('Sales')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Y1UkK95Rkh2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter Plot with items and sq. ft to see if there is a linear correlation\n",
        "plt.scatter(store_sales_copy_df['Items_Available'], store_sales_copy_df['sq_ft'])\n",
        "\n",
        "# Set the title and axis labels\n",
        "plt.title('Items Available vs. Square Feet')\n",
        "plt.xlabel('Items Available')\n",
        "plt.ylabel('Square Feet')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "K650Ss6ClCAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create correlations df\n",
        "\n",
        "Correlation_df = store_sales_df[['sq_ft', 'Items_Available', 'Daily_Customer_Count', 'Store_Sales']]\n"
      ],
      "metadata": {
        "id": "olJLhPggpujF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import seaboarn as sns\n",
        "\n",
        "!pip install seaborn\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "ZZ4mDIDGoOFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_theme(style=\"ticks\",palette='deep')\n",
        "sns.pairplot(Correlation_df)"
      ],
      "metadata": {
        "id": "JD25HWP7oIkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression Model\n"
      ],
      "metadata": {
        "id": "xqVsxSq2Prqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# Extract features (X) and target variable (y)\n",
        "X = store_sales_copy_df[['Items_Available', 'sq_ft', 'Daily_Customer_Count','customer_per_item','items_per_sq_ft','customer_per_sq_ft']]\n",
        "y = store_sales_copy_df['Store_Sales']\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "v3x0iubfS4Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "Z-zJ8APnTkHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "#Training score\n",
        "print(f\"Training Score: {model.score(X_train_scaled, y_train)}\")\n",
        "# MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "# Calculate the RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, model.predict(X_test_scaled)))\n",
        "print(\"RMSE:\", rmse)\n",
        "# R2\n",
        "r2 = metrics.r2_score(y_test, y_pred)\n",
        "print(f\"R2 score: {r2}\")\n",
        "# Visualize predicted vs. actual values\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel('Actual Store Sales')\n",
        "plt.ylabel('Predicted Store Sales')\n",
        "plt.title('Actual vs. Predicted Store Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EReDrdQ9OP7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance\n",
        "feature_importance = model.coef_\n",
        "print('Feature Importance:', feature_importance)"
      ],
      "metadata": {
        "id": "0-fv6dEUipFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest\n"
      ],
      "metadata": {
        "id": "8piwbibKh6-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# Extract features (X) and target variable (y)\n",
        "X = store_sales_copy_df[['Items_Available', 'sq_ft', 'Daily_Customer_Count','customer_per_item','items_per_sq_ft','customer_per_sq_ft']]\n",
        "y = store_sales_copy_df['Store_Sales']\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "wCS8U9nph6tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "# Model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Model Evaluation\n",
        "y_pred = rf_model.predict(X_test)\n",
        "# Evaluate the model using metrics like Mean Squared Error and R-squared\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'R-squared: {r2}')\n",
        "\n",
        "# Feature Importance\n",
        "feature_importance = rf_model.feature_importances_\n",
        "print('Feature Importance:', feature_importance)"
      ],
      "metadata": {
        "id": "5ciev-dNiFRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = rf_model.predict(X_test_scaled)\n",
        "#Training score\n",
        "print(f\"Training Score: {model.score(X_train_scaled, y_train)}\")\n",
        "# MSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "# Calculate the RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, model.predict(X_test_scaled)))\n",
        "print(\"RMSE:\", rmse)\n",
        "# R2\n",
        "r2 = metrics.r2_score(y_test, y_pred)\n",
        "print(f\"R2 score: {r2}\")\n",
        "# Visualize predicted vs. actual values\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel('Actual Store Sales')\n",
        "plt.ylabel('Predicted Store Sales')\n",
        "plt.title('Actual vs. Predicted Store Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p-hfj5bjh6C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost Model"
      ],
      "metadata": {
        "id": "VXowF1LjPxmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Models\n",
        "from xgboost.spark import SparkXGBRegressor\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score"
      ],
      "metadata": {
        "id": "-pwo272KcBrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Extract features (X) and target variable (y)\n",
        "X = store_sales_copy_df[['Items_Available', 'sq_ft', 'Daily_Customer_Count','customer_per_item','items_per_sq_ft','customer_per_sq_ft']]\n",
        "y = store_sales_copy_df['Store_Sales']\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "CVMkRHCNTsLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#xgboost model\n",
        "xgbr = XGBRegressor(random_state = 116)\n",
        "xgbr.fit(X_train_scaled, y_train)\n",
        "xgbr_pred =xgbr.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "XE2n9-mebtBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "mse = mean_squared_error(y_test, xgbr_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, xgbr_pred))\n",
        "score = xgbr.score(X_train, y_train)\n",
        "r2 =r2_score(y_test,  xgbr_pred)"
      ],
      "metadata": {
        "id": "MCb2INjOcQSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training score: \", score)\n",
        "print(\"MSE: %.2f\" % mse)\n",
        "print(\"RMSE: %.2f\" % (rmse))\n",
        "print(\"R_score : %.2f\"%(r2))\n",
        "# Visualize predicted vs. actual values\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel('Actual Store Sales')\n",
        "plt.ylabel('Predicted Store Sales')\n",
        "plt.title('Actual vs. Predicted Store Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kvpxJDRccsrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN Model"
      ],
      "metadata": {
        "id": "QQ3kq4gJP5h6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# Extract features (X) and target variable (y)\n",
        "X = store_sales_copy_df[['Items_Available', 'sq_ft', 'Daily_Customer_Count']]\n",
        "y = store_sales_copy_df['Store_Sales']\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "cTN3M_CpTw1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=3, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test))"
      ],
      "metadata": {
        "id": "JLm9G37Yd1NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize training history\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QYarh15RN5C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "#MSE\n",
        "mse = mean_squared_error(y_test, model.predict(X_test_scaled))\n",
        "print(\"MSE:\", mse)\n",
        "# Calculate the RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, model.predict(X_test_scaled)))\n",
        "print(\"RMSE:\", rmse)\n",
        "# R2\n",
        "r2 = metrics.r2_score(y_test, y_pred)\n",
        "print(f\"R2 score: {r2}\")\n",
        "# Visualize predicted vs. actual values\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel('Actual Store Sales')\n",
        "plt.ylabel('Predicted Store Sales')\n",
        "plt.title('Actual vs. Predicted Store Sales')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5TDD8xStN1I6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}